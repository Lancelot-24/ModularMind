,topic,content
0,Reasoning via Planning (RAP)_0,"In this section, we present the Reasoning via Planning (RAP) framework that enables LLMs to strategically plan a coherent reasoning trace for solving a wide range of reasoning tasks. We first build the world model by repurposing the LLM with prompting (Section 3.1). The world model serves as the foundation for deliberate planning, by allowing the LLM to plan ahead and seek out the expected outcomes in the future. We then introduce the rewards for assessing each state during reasoning in Section 3.2. Guided by the world model and rewards, the planning with Monte Carlo Tree Search (MCTS) efficiently explores the vast reasoning space and finds optimal reasoning traces (Section 3.3). Finally, when multiple promising reasoning traces are acquired during planning, we further introduce an aggregation method in Section 3.4 that yields an integrated result and further boosts the reasoning performance. 3.1"
1,Language Model as World Model_0,"In general, a world model predicts the next state of the reasoning after applying an action to the current state [ 17,39]. RAP enables us to instantiate the general concepts of state and action in different ways depending on the specific reasoning problems at hand. For example, in Blocksworld (Figure 2 left), it is natural to set a state to describe a configuration of blocks (with natural language), and an action to be a behavior of moving a block (e.g., ‚Äúpickup the orange block‚Äù ). In a math reasoning problem (Figure 2 right), we use the state to represent the values of intermediate variables, and set an action to be a subquestion that drives the reasoning to derive new values (i.e., new state). After the definition of state and action, the reasoning process can thus be described as a Markov decision process (MDP): given the current state st,t=0,1,...,T, e.g., the initial state s0, the LLM (as a reasoning agent) generates an action following its generative distribution at‚àºp(a|st, c), where cis a proper prompt (e.g., in-context demonstrations) to steer the LLM for action generation. The world model then predicts the next state st+1of the reasoning. Specifically, we repurpose the same LLM to obtain a state transition distribution p(st+1|st, at, c‚Ä≤), where c‚Ä≤is another prompt to guide the LLM to generate a state. For instance, in Blocksworld, the LLM (as the world model) generates text st+1 to describe the new configuration of blocks, given the previous state description stand the action at. Continuing the process results in a reasoning trace, which consists of a sequence of interleaved states and actions (s0, a0, s1, . . . , a T‚àí1, sT). This differs from the previous reasoning methods, such as Chain-of-Thought [ 59], where the intermediate reasoning steps consist of only a sequence of actions, e.g., ( a0=‚Äúpickup red block‚Äù, a1=‚Äústack on yellow block‚Äù, . . . ) (see comparisons in Figure 1). Augmenting the reasoning with the (predicted) world states helps the LLM with a more grounded and coherent inference. Note that the full reasoning trace is simulated by the LLM itself (as a reasoning agent with an internal world model) without interacting with the external real environment. This resembles humans contemplating a possible plan in their minds. The capability of simulating future states, due to the introduction of the world model, allows us to incorporate principled planning algorithms to efficiently explore the vast reasoning space as described in Section 3.3. 3.2"
2,Reward Design_0,"During reasoning, we want to assess the feasibility and desirability of each reasoning step, and guide the reasoning based on the assessment (Section 3.3). The assessment of each reasoning step (i.e., applying an action atto the state st) is performed by a reward function rt=r(st, at)‚ààR. 4PickuporangePickupblueStackiton blueStackiton orange‚Ä¶PickuporangePickupblueStackiton blueStackiton orange‚Ä¶PickuporangePickupredPickuporangePickupblueStackiton blueStackiton orange‚Ä¶PickuporangePickupredQ ‚Ä¶QQQQQ reward(a) Selection(b) Expansion(c) Simulation(d) Back-propagationQùë†!ùëé!ùë†""ùëé""ùë†#ùëé#ùë†$ùë†%‚Ä¶Figure 3: An illustration of the four phases in an iteration in MCTS planning (Section 3.3). Similar to the state and action, the reward function can be specified in different ways to accommodate any knowledge or preferences about the reasoning problem of interest. Here we introduce several common rewards applicable to different tasks and shown to be effective in our experiments. Likelihood of the action. When an action is generated by the LLM conditioning on the current state, the probability of the specific action reflects the LLM‚Äôs preference. We thus can incorporate the log probability of the action as a reward. Confidence of the state. State prediction is nontrivial in some problems, e.g., in math reasoning (Figure 2, right), given an action (i.e., a subquestion), the world model predicts the next state by answering the subquestion. We incorporate the confidence of the state (i.e., answers in this case) as a reward. Specifically, we draw multiple sample answers from the world model, and use the proportion of the most frequent answer as the confidence. A high confidence indicates a reliable reasoning step. Self-evaluation by the LLM. We can also let the LLM criticize itself with the question ‚Äú Is this reasoning step correct? ‚Äù and use the next-word probability of the token ‚Äú Yes‚Äù as a reward. The reward evaluates LLM‚Äôs own estimation of the correctness of reasoning. Similarly, we can get another reward by prompting with the question ‚Äú Is this reasoning step helpful? ‚Äù, which is a self-evaluation by LLM on the helpfulness of a reasoning step towards the target. Task-specific heuristics. We can also flexibly plug-in other diverse task-specific heuristics into the reward function. For example, in plan generation for Blocksworld, we compare the current predicted state of blocks with the goal to calculate a reward (Section 4.1). The reward encourages the plan of movements to actively pace towards the target. 3.3"
3,Planning with Monte Carlo Tree Search_0,"Once equipped with the world model (Section 3.1) and rewards (Section 3.2), we can enable LLMs to reason with advanced planning algorithms, where we adopt Monte Carlo Tree Search (MCTS) [ 31,12], a powerful planning algorithm that strategically explores the space of reasoning trees and strikes a proper balance between exploration and exploitation to find high-reward reasoning traces efficiently. MCTS builds a reasoning tree iteratively, where each node represents a state, and each edge represents an action and the transition from the current state to the next state after applying the action (Figure 1). To guide the LLM agent to expand and explore the most promising nodes of the tree, the algorithm maintains a state-action value function Q:S √ó A 7‚Üí R, where Q(s, a)estimates the expected future reward of taking action ain state s. That is, we assess the potential of a node (or a reasoning step) by looking ahead and anticipating the reward in future trajectories starting from this node. This fundamentally differs from the current reasoning methods that generate a reasoning trace autoregressively from left to right without accounting for the future. More specifically, as illustrated in Figure 3, the MCTS planning performs four operations in each iteration to expand the tree and update Qvalues, i.e., selection ,expansion ,simulation , and back- propagation . The process continues until a specified computational budget (e.g., the number of 5iterations) is reached, and the resulting reasoning traces are acquired from the tree, as we articulated later. The psuedo-code of our MCTS planning is given in Algorithm 1 in the Appendix. Selection. The first phase selects a portion of the existing tree that is most promising for further expansion in the next phase. Specifically, starting from the root node (i.e., initial state s0), at each level of the tree, the algorithm selects a child node as the next node. The phase finishes when a leaf node of the current tree is reached. Figure 3(a) highlights the selected path in red. To balance between exploration (of less-visited nodes) and exploitation (of high-value nodes), we use the well- known Upper Confidence bounds applied to Trees (UCT) algorithm [ 31] to select each child node. Specifically, at node s, we select the action (which leads to a transition to a child node) in the tree by considering both the Qvalue (for exploitation) and uncertainty (for exploration): a‚àó= arg max a‚ààA(s)"" Q(s, a) +ws lnN(s) N(c(s, a))# , (1) where N(s)is the number of times node shas been visited in previous iterations, and c(s, a)is the child node of applying ain state s. Therefore, the less a child node was visited before (i.e., the more uncertain about this child node), the higher the second term in the equation. The weight wcontrols the balance between exploration and exploitation. Expansion. This phase expands the tree by adding new child nodes to the leaf node selected above. Specifically, given the state of the leaf node, we use the LLM (as agent) to sample dpossible actions (e.g., subquestions in math reasoning), and then use the LLM (as world model) to predict the respective next state, resulting in dchild nodes. From the dnodes, we pick the node of largest local reward (Section 3.2) for simulation in the next phase. Note that if the leaf node selected above is a terminal (target) state already, we will skip expansion/simulation and jump to back-propagation. Simulation. This phase simulates the future situations of the current node using the world model, in order to estimate the expected future rewards ( Qvalues). Specifically, starting from the current node as above, at each node s, we create an action following a roll-out policy and use the world model to predict the next state. The roll-out process continues until a terminal state if reached. There could be different ways to define the roll-out policy (e.g., by adding different randomness). In our experiments, for simplicity and reduced noises, we just follow the same process as in the expansion above, by generating dcandidate actions and picking one of the largest local reward a‚Ä≤= max a‚Ä≤r(s, a). In practice, as the roll-out process will evaluate the reward function for multiple nodes, for efficiency, we discard the computationally expensive components in r(for example, the reward from the confidence of state requires sampling the answer multiple times), and use the resulting light-weight reward function for selecting actions during simulation. Back-propagation. Once we reach a terminal state in the above phases, we obtain a reasoning path from the root node to the terminal node. We now back-propagate the rewards on the path to update theQvalue of each state-action pair along the path. That is, Q(s, a)is updated by aggregating the rewards in all future steps of node s. We may adopt the aggregation method according to the nature of different tasks and reward design, as discussed in Section 4. As mentioned earlier, once a predetermined number of MCTS iterations is reached, we terminate the algorithm and select final reasoning trace from the constructed tree. There could be various ways for the selection. One approach is to start from the root node and iteratively choose the action with the highest Qvalue until reaching a terminal. Alternatively, one can directly select the path from the iterations that yielded the highest reward, or opt to choose the leaf node (and the respective root-to-leaf path) that has been visited the most. In practice, we observed that the second strategy often yields the best results. 3.4"
4,RAP-Aggregation: Aggregating Multiple Reasoning Outputs_0,"Ensemble-based methods, such as self-consistency CoT [ 58], can effectively improve performance by aggregating multiple valid reasoning traces. Therefore, for problems, such as math reasoning (Section 4.2) where only the final answer is required, RAP could produce multiple traces and answers from different MCTS iterations, which will be aggregated to produce the final answer. We refer to such a mechanism as RAP-Aggregation. Note that problems like plan generation or logical inference require a complete reasoning trace as output; thus, RAP-Aggregation will not be applied. 6More importantly, there is a concern that some incorrect reasoning steps may appear in the early stage of multiple iterations, thus polluting the aggregation. As a result, we further devise a new weighting strategy for aggregating candidate answers. Specifically, for each candidate answer, we accumulate the reward of each reasoning step in the answer‚Äôs reasoning traces. We choose the answer with the highest accumulative reward as the final aggregated answer. 4"
